{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EXT_ATH import process_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = \"postgresql+psycopg://postgres:password@172.31.41.128:5432/db_stage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_sql = \"\"\"select\n",
    "\tcurrent_date as curr_date,\n",
    "\tdate(date_parse(\"O_Created Date & Time\", '%Y-%m-%dT%H:%i:%s')) as ord_date,\n",
    "\t\"seller np name\" AS seller_np,\n",
    "    SUM(CASE WHEN \"Fulfillment Id\" IS NULL THEN 1 ELSE 0 END) AS null_fulfilment_id,\n",
    "    SUM(CASE WHEN \"Network Transaction Id\" IS NULL THEN 1 ELSE 0 END) AS null_net_tran_id,\n",
    "    SUM(CASE WHEN \"Qty\" IS NULL THEN 1 ELSE 0 END) AS null_qty,\n",
    "    SUM(CASE WHEN \"Item Fulfillment Id\" IS NULL THEN 1 ELSE 0 END) AS null_itm_fulfilment_id,\n",
    "    SUM(CASE WHEN \"Delivery Pincode\" IS NULL OR 'Delivery Pincode' LIKE '%XXX%' THEN 1 ELSE 0 END) AS null_del_pc,\n",
    "    SUM(CASE WHEN \"O_Created Date & Time\" IS NULL THEN 1 ELSE 0 END) AS null_created_date_time,\n",
    "    SUM(CASE WHEN \"Domain\" IS NULL THEN 1 ELSE 0 END) AS null_domain,\n",
    "    SUM(CASE WHEN \"Delivery City\" IS NULL OR \"Delivery City\" LIKE '%XXX%' THEN 1 ELSE 0 END) AS null_del_cty,\n",
    "    SUM(CASE WHEN (\"Order Status\"='Cancelled') AND (\"Cancellation Code\" IS NULL  OR \"Cancellation Code\" LIKE '%Item Out of Stock%' OR \"Cancellation Code\" LIKE '%std:011%')THEN 1 ELSE 0 END) AS null_cans_code,\n",
    "    SUM(CASE WHEN \"Order Status\"='Cancelled' AND \"F_Cancelled At Date & Time\" IS NULL THEN 1 ELSE 0 END) AS null_cans_dt_time,\n",
    "    SUM(CASE WHEN \"Order Status\" IS NULL THEN 1 ELSE 0 END) AS null_ord_stats,\n",
    "    SUM(CASE WHEN \"Fulfillment Status\" IS NULL THEN 1 ELSE 0 END) AS null_fulfil_status,\n",
    "    SUM(CASE WHEN \"Item Category\" IS NULL THEN 1 ELSE 0 END) AS null_itm_cat,\n",
    "    SUM(CASE WHEN \"Item Consolidated Category\" IS NULL THEN 1 ELSE 0 END) AS null_cat_cons,\n",
    "    SUM(CASE WHEN \"Seller Pincode\" IS NULL OR \"Seller Pincode\" LIKE '%XXXX%' THEN 1 ELSE 0 END) AS null_sell_pincode,\n",
    "    SUM(CASE WHEN \"Provider id\" IS NULL THEN 1 ELSE 0 END) AS null_prov_id,\n",
    "    SUM(CASE WHEN \"item id\" IS NULL THEN 1 ELSE 0 END) AS null_itm_id,\n",
    "    SUM(CASE WHEN \"seller np name\" IS NULL THEN 1 ELSE 0 END) AS null_sell_np,\n",
    "    SUM(CASE WHEN \"network order id\" IS NULL THEN 1 ELSE 0 END) AS null_net_ord_id,\n",
    "    SUM(CASE WHEN \"seller city\" IS NULL THEN 1 ELSE 0 END) AS null_sell_cty,\n",
    "    count(*) as total_orders,\n",
    "    SUM(CASE WHEN \"Order Status\" = 'Cancelled' THEN 1 ELSE 0 END) AS total_canceled_orders\n",
    "FROM default.nhm_order_fulfillment_subset_v1\n",
    "where\n",
    "\tdate(date_parse(\"O_Created Date & Time\", '%Y-%m-%dT%H:%i:%s')) = date('{date_val}')\n",
    "GROUP by\n",
    "\tdate(date_parse(\"O_Created Date & Time\", '%Y-%m-%dT%H:%i:%s')),\n",
    "\t\"seller np name\";\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_results(results):\n",
    "    final_data = []\n",
    "    columns = [x[\"VarCharValue\"] for x in results[\"rows\"][0][\"Data\"]]\n",
    "    rows = [list(map(lambda field: field.get('VarCharValue', ''), row['Data'])) for row in results['rows'][1:]]\n",
    "    for data in rows:\n",
    "        final_data.append(data)\n",
    "    return pd.DataFrame(columns=columns, data=final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dates(start_date):\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.now().date()\n",
    "    date_list = []\n",
    "    \n",
    "    while start_date.date() <= end_date:\n",
    "        date_list.append(start_date.date())\n",
    "        start_date += timedelta(days=1)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2024-05-01'\n",
    "dates_between = list_dates(start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for date_val in dates_between:\n",
    "#     print(f\"Processing {date_val}\")\n",
    "#     results = await process_date(tbl_name=\"nhm_order_fulfillment_subset_v1\",date=date_val,raw_query=dq_sql)\n",
    "#     df = get_raw_results(results)\n",
    "#     df.to_parquet(f\"D:\\\\Work\\\\git\\\\Data-Quality\\\\FA_Backend\\\\Data\\\\data_quality_{date_val}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection information\n",
    "conn_info = {\n",
    "    'dbname': 'db_stage',\n",
    "    'user': 'postgres',\n",
    "    'password': 'password',\n",
    "    'host': '172.31.41.128',\n",
    "    'port': '5432'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import psycopg\n",
    "from io import StringIO\n",
    "\n",
    "def dump_parquet_to_postgresql(parquet_file_path, table_name, conn_info):\n",
    "    table = pq.read_table(parquet_file_path)\n",
    "    df = table.to_pandas()\n",
    "    try:\n",
    "        with psycopg.connect(**conn_info) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Use COPY to dump data into PostgreSQL\n",
    "                buffer = StringIO()\n",
    "                df.to_csv(buffer, index=False, header=False)\n",
    "                buffer.seek(0)\n",
    "                cur.copy(\"COPY {} FROM STDIN WITH CSV\".format(table_name), buffer)\n",
    "                conn.commit()\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    else:\n",
    "        print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = \"D:\\\\DATA_DUMP\\\\Data_quality\\\\*.parquet\"\n",
    "table_name = \"od_dq_nhm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob(data_loc)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(glob(data_loc)[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"curr_date\"] = pd.to_datetime(df[\"curr_date\"], format=\"%Y-%m-%d\")\n",
    "df[\"ord_date\"] = pd.to_datetime(df[\"ord_date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into Postgresql \n",
    "# ===========================================\n",
    "\n",
    "# for file in glob(data_loc):\n",
    "#   print(f\"Processing {file}.\")\n",
    "#   df = pd.read_parquet(file)\n",
    "#   df[\"curr_date\"] = pd.to_datetime(df[\"curr_date\"], format=\"%Y-%m-%d\")\n",
    "#   df[\"ord_date\"] = pd.to_datetime(df[\"ord_date\"], format=\"%Y-%m-%d\")\n",
    "#   df.to_sql(\n",
    "# \tname=\"od_dq_nhm\",\n",
    "# \tcon= conn_str,\n",
    "# \tindex=False,\n",
    "# \tif_exists='append',\n",
    "# \tschema=\"ec2_all\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet_file_path = 'your_file.parquet'\n",
    "# table_name = 'your_table_name'\n",
    "\n",
    "# dump_parquet_to_postgresql(parquet_file_path, table_name, conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest of the Tables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
